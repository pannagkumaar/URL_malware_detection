import re
from urllib.parse import urlparse
import requests
import difflib
from datetime import datetime

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import pickle
import warnings
from urllib.parse import urlparse
import re
from bs4 import BeautifulSoup as bsoup
from rich.console import Console
from rich.table import Table
from fake_useragent import UserAgent
from colorama import Fore, init
init()

def check_redirects(url, verbosity=True):
    class URLRedirectionChecker:
        def __init__(self, url):
            self.url = url
            self.expanded_url = None
            self.target_ip_address = None
            self.servers = []

        def get_user_agent(self):
            ua = UserAgent()
            return ua.random

        def get_domain_name(self, url: str) -> str:
            url_parts = url.split('/')
            return url_parts[2]

        def check_url_redirections(self, verbosity=True):
            headers = {
                'Accept-Encoding': 'gzip, deflate, br',
                'User-Agent': self.get_user_agent(),
                'Referer': 'https://iplogger.org/',
                'DNT': '1',
                'Upgrade-Insecure-Requests': '1',
            }

            ip_logger_url_checker = "https://iplogger.org/url-checker/"

            with requests.Session() as session:
                try:
                    response = session.get(ip_logger_url_checker, headers=headers)
                    if 'Set-Cookie' in response.headers:
                        headers['Cookie'] = response.headers['Set-Cookie']
                    if 'Cache-Control' in response.headers:
                        headers['Cache-Control'] = response.headers['Cache-Control']
                    if 'Last-Modified' in response.headers:
                        headers['If-Modified-Since'] = response.headers['Last-Modified']

                    params = {"url": self.url}
                    response = session.get(ip_logger_url_checker, headers=headers, params=params)

                    self.servers = []
                    if response.ok:
                        soup = bsoup(response.content, 'html.parser')
                        servers_info = soup.find_all("div", class_="server-info")

                        for server_info in servers_info:
                            server_items = server_info.find_all("div", class_="server-item")
                            server_antivirus = server_info.find("div", class_="server-antivirus")
                            server_next = server_info.find("div", class_="server-next")
                            server_item_info = {}

                            for server_item in server_items:
                                if server_item.contents and len(server_item.contents) == 2:
                                    key, value = server_item.contents[0].string.strip(), server_item.contents[1].string.strip()
                                    server_item_info[key] = value

                            server_item_info["Status code"] = server_next.contents[1].string.strip()
                            server_item_info["Google Safe Browsing Database"] = server_antivirus.contents[1].string.strip()
                            self.servers.append(server_item_info)

                    self.display_results(verbosity)
                except requests.RequestException as e:
                    print(f"Error checking redirects for {self.url}: {str(e)}")

        def display_results(self, verbosity):
            number_of_redirections = len(self.servers)
            console = Console()

            if verbosity and number_of_redirections > 1:
                table = Table(title="â„ ð”¼ ð”» ð•€ â„ ð”¼ â„‚ ð•‹ ð•€ ð•† â„• ð•Š", show_lines=True)
                table.add_column("ID", justify="center")
                table.add_column("URL", justify="center", max_width=60)
                table.add_column("Status Code", justify="center")
                table.add_column("IP Address", justify="center")
                table.add_column("Country by IP", justify="center")

                for server_index, server_info in enumerate(self.servers, start=1):
                    table.add_row(
                        str(server_index),
                        server_info.get('Host', ''),
                        server_info.get('Status code', ''),
                        server_info.get('IP address', ''),
                        server_info.get('Country by IP', '')
                    )

                console.print(table)
            elif number_of_redirections > 1:
                table = Table(title="â„ ð”¼ ð”» ð•€ â„ ð”¼ â„‚ ð•‹ ð•€ ð•† â„• ð•Š", show_lines=True)
                table.add_column("Source URL", justify="center", max_width=60)
                table.add_column("Source Domain", justify="center")
                table.add_column("Destination URL", justify="center", max_width=60)
                table.add_column("Destination Domain", justify="center")
                table.add_row(
                    self.url,
                    self.get_domain_name(self.url),
                    self.expanded_url if self.expanded_url else '',
                    self.get_domain_name(self.expanded_url) if self.expanded_url else ''
                )

                console.print(table)
            else:
                console.print('[green3][green3][-][/green3][/green3] No redirection found!')

    # Instantiate and use the class
    checker = URLRedirectionChecker(url)
    checker.check_url_redirections(verbosity)

def sanitization(web):
    web = web.lower()
    tokens = set()
    raw_tokens = set()

    for part in web.split('/'):
        raw_tokens.update(part.split('-'))
        raw_tokens.update(part.split('.'))

    tokens.update(raw_tokens)
    tokens.discard('com')

    return list(tokens)

def load_model(file_path):
    with open(file_path, 'rb') as file:
        model = pickle.load(file)
    return model

def load_vectorizer(file_path):
    with open(file_path, 'rb') as file:
        vectorizer = pickle.load(file)
    return vectorizer

def predict_url(model, vectorizer, url):
    x = vectorizer.transform([url])
    prediction = model.predict(x)[0]
    return prediction

def predict(url_to_check):
    warnings.filterwarnings("ignore")

    

    # Whitelist
    whitelist=open("./file.txt",'r').readlines()

    # Filtering URL
    if url_to_check in whitelist:
        prediction = 'good'
    else:
        # Loading Model and Vectorizer
        model_path = "Classifier/pickel_model_lgr.pkl"
        vectorizer_path = "Classifier/pickel_vector.pkl"
        lgr_model = load_model(model_path)
        tfidf_vectorizer = load_vectorizer(vectorizer_path)

        # Predicting
        prediction = predict_url(lgr_model, tfidf_vectorizer, url_to_check)

    warnings.resetwarnings()
    return prediction
    
def is_malicious_url(url):
    console=Console()
    console.print("[*] Analyzing the URL...\n")

    # Parse the URL
    parsed_url = urlparse(url)

    prediction = predict(url)
    prediction_label =  + '[green3]GOOD[/green3]'  if prediction == 'good' else  '[red3]MALICIOUS[/red3]' 
    console.print(f"[red3][-][/red3] The entered domain is: {prediction_label}")

    if prediction != 'good':
        console.print("[green3][-][/green3] If you feel that this prediction is wrong or are unsure about the output, you can contact us at harshith007varma007@gmail.com or pannag2003@gmail.com. We'll check the URL and update the model accordingly. Thank you.")

    # Check for phishing keywords
    potential_typosquatting_urls = check_typosquatting("file.txt", url)

    if potential_typosquatting_urls:
        console.print("[green3][-][/green3] Potential typosquatting URLs found:\n")
        for entry in potential_typosquatting_urls:
            console.print(f"    - URL: {entry['url']} (Similarity: {entry['similarity']:.2f})")
    else:
        console.print("[red3][-][/red3] No potential typosquatting URLs found.\n")
        
    phishing_keywords = ['login', 'password', 'account', 'verify', 'secure']
    phishing_result = any(keyword in parsed_url.path.lower() for keyword in phishing_keywords)
    console.print(f"[green3][-][/green3] Phishing keywords check: {'Detected' if phishing_result else 'Not detected'}\n")

    # Check for IP address in the URL
    ip_result = bool(re.match(r'\d+\.\d+\.\d+\.\d+', parsed_url.netloc))
    console.print(f"[green3][-][/green3] IP address check: {'Detected' if ip_result else 'Not detected'}\n")

    # Check for non-standard ports
    port_result = parsed_url.port and parsed_url.port not in [80, 443]
    console.print(f"[green3][-][/green3] Non-standard ports check: {'Detected' if port_result else 'Not detected'}\n")

    # Check for unusual URL structure
    structure_result = check_unusual_structure(parsed_url.path)
    console.print(f"[green3][-][/green3] Unusual URL structure check: {'Detected' if structure_result else 'Not detected'}\n")

    # Check for redirect chains
    check_redirects(url)
    
    # Check for HTTPS usage anomalies
    https_anomalies_result = check_https_anomalies(url)
    console.print(f"[green3][-][/green3] HTTPS anomalies check: {'Detected' if https_anomalies_result else 'Not detected'}\n")

    # Check for dynamic parameters
    dynamic_params_result = check_dynamic_parameters(parsed_url.query)
    console.print(f"[green3][-][/green3] Dynamic parameters check: {'Detected' if dynamic_params_result else 'Not detected'}\n")


def check_typosquatting(file_path, input_url, threshold=0.75):
    if input_url.startswith("http://"):
        input_url = input_url[7:]
    elif input_url.startswith("https://"):
        input_url = input_url[8:]
       
    def sequence_matcher_similarity(text1, text2):
        seq_matcher = difflib.SequenceMatcher(None, text1, text2)
        return seq_matcher.ratio()

    with open(file_path, "r") as file:
        urls = [line.strip() for line in file.readlines()]
        

    input_url = input_url.lower().replace("www.", "")

    potential_typosquatting = []

    for url in urls:
        url = url.lower().replace("www.", "")
        similarity = sequence_matcher_similarity(input_url, url)
        if similarity >= threshold:
            potential_typosquatting.append({"url": url, "similarity": similarity})

    return potential_typosquatting

def has_shortening_service(url):
    pattern = re.compile(r'https?://(?:www\.)?(?:\w+\.)*(\w+)\.\w+', re.IGNORECASE)
    match = pattern.search(url)
    
    if match:
        domain = match.group(1).lower()
        common_shortening_services = {'bit', 'goo', 'tinyurl', 'ow', 't', 'is',
                                      'cli', 'yfrog', 'migre', 'ff', 'url4', 'twit',
                                      'su', 'snipurl', 'short', 'BudURL', 'ping', 
                                      'post', 'Just', 'bkite', 'snipr', 'fic', 
                                      'loopt', 'doiop', 'short', 'kl', 'wp', 
                                      'rubyurl', 'om', 'to', 'bit', 't', 'lnkd', 
                                      'db', 'qr', 'adf', 'goo', 'bitly', 'cur', 
                                      'tinyurl', 'ow', 'bit', 'ity', 'q', 'is', 
                                      'po', 'bc', 'twitthis', 'u', 'j', 'buzurl', 
                                      'cutt', 'u', 'yourls', 'x', 'prettylinkpro', 
                                      'scrnch', 'filoops', 'vzturl', 'qr', '1url', 
                                      'tweez', 'v', 'tr', 'link', 'zip'}
        
        if domain in common_shortening_services:
            return 1
    return 0




def check_unusual_structure(path):
    # Check for unusual URL structure
    # Implement your own logic based on path analysis

    # 1. Long Sequences of Repeating Characters
    consecutive_repeating_chars = re.search(r'(.)\1{5,}', path)
    if consecutive_repeating_chars:
        return True

    # 2. Uncommon Symbols
    uncommon_symbols = set(path) - set('abcdefghijklmnopqrstuvwxyz0123456789-._/')
    if uncommon_symbols:
        return True

    # 3. Excessive Use of Hyphens or Underscores
    excessive_hyphens_underscores = re.search(r'[-_]{4,}', path)
    if excessive_hyphens_underscores:
        return True

    # 4. Unusual Path Length
    if len(path) > 50:
        return True

    # 5. Presence of Hexadecimal Encoding
    if re.search(r'%[0-9A-Fa-f]{2}', path):
        return True

    # 6. Uncommon File Extensions
    uncommon_extensions = re.search(r'\.\w{4,}', path)
    if uncommon_extensions:
        return True

    # If none of the checks found anything unusual
    return False



# def has_redirects(url, max_redirects=5, timeout=5, user_agents=None, whitelist=None):
#     if user_agents is None:
#         user_agents = [
#             'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
#             # Add more user agents as needed
#         ]

#     headers = {'User-Agent': user_agents[0]}

#     try:
#         response = requests.head(url, allow_redirects=True, timeout=timeout, headers=headers)

#         # Check if there are multiple redirects
#         if len(response.history) > max_redirects:
#             return True

#         # Check the final HTTP status code
#         if not (200 <= response.status_code < 300):
#             return False

#         # Follow all redirects and check the final destination
#         final_url = response.url
#         if whitelist and any(domain in final_url for domain in whitelist):
#             return False

#         # Implement additional checks, such as pattern matching or threat intelligence integration

#     except requests.RequestException as e:
#         # Handle request exceptions (e.g., connection timeout, invalid URL)
#         print(f"Error checking redirects for {url}: {str(e)}")

#     return False

def check_https_anomalies(url):
    try:
        # Ensure the URL starts with 'https://'
        if not url.startswith('https://'):
            print("URL does not use HTTPS.")
            return True

        # Retrieve SSL certificate
        response = requests.head(url, allow_redirects=True, timeout=5)
        if response.status_code == 200 and response.url.startswith('https://'):
            parsed_url = urlparse(url)
            domain = parsed_url.netloc

            # Check if the SSL certificate is valid
            cert = response.connection.sock.getpeercert()
            not_after = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
            if not_after < datetime.now():
                print("SSL certificate has expired.")
                return True

            # Check if the common name matches the domain
            common_name = cert['subject'][0][0][1]
            if not common_name.lower() == domain.lower():
                print(f"Common name in SSL certificate does not match the domain: {common_name}")
                return True

            # Check if the certificate is issued by a trusted authority
            issuer = cert['issuer'][0][0][1]
            trusted_authorities = ['Let\'s Encrypt', 'DigiCert', 'Comodo', 'Symantec', 'GeoTrust']
            if not any(authority.lower() in issuer.lower() for authority in trusted_authorities):
                print(f"Certificate issuer is not from a trusted authority: {issuer}")
                return True

            # Add more checks as needed based on your specific requirements

    except requests.exceptions.RequestException as e:
        print(f"Error checking HTTPS anomalies: {e}")

    return False

def check_dynamic_parameters(query, threshold=3):
    # Check for dynamic parameters
    # Implement more sophisticated logic based on query analysis

    # Split the query string into individual parameters
    parameters = [param.split('=') if '=' in param else (param, '') for param in query.split('&')]

    # Check the total number of parameters
    if len(parameters) > threshold:
        return True

    # Check for specific patterns in parameter names or values
    for param in parameters:
        # Ensure that the parameter has both a name and a value
        if len(param) == 2:
            # Check for numeric values (considering decimals)
            if re.match(r'^[+-]?\d*\.?\d+$', param[1]):
                return True

            # Check for suspicious characters in parameter names or values
            if re.search(r'[^\w.-]', param[0]) or re.search(r'[^\w.-]', param[1]):
                return True

            # Improve SQL injection detection
            if re.search(r'\b(union|select|insert|update|delete|alter|drop|truncate|exec)\b', param[1], re.IGNORECASE):
                return True

            # Enhance XSS detection
            if re.search(r'<\s*/?\s*script\s*>', param[1], re.IGNORECASE):
                return True

            # Check for parameter names commonly used in attacks
            if param[0].lower() in ['script', 'exec', 'cmd', 'system', 'eval']:
                return True

            # Check for excessively long parameter values
            if len(param[1]) > 50:
                return True

        # Add more checks based on your specific requirements

    # If none of the checks found anything suspicious
    return False

def is_suspicious_tld(domain):
    # Make the comparison case-insensitive
    domain_lower = domain.lower()

    # List of suspicious TLDs
    suspicious_tlds = [
        '.tk', '.ml', '.ga', '.cf', '.gq',
        '.pw', '.to', '.cc', '.ws', '.su',
        '.buzz', '.download', '.guru', '.loan', '.racing',
        '.review', '.top', '.vip', '.win', '.work',
        '.country', '.stream', '.download', '.xin', '.gdn',
        '.jetxt', '.bid', '.vip', '.ren', '.kim',
        '.loan', '.mom', '.party', '.review', '.trade',
        '.date', '.wang', '.accountants'
        # Add more TLDs as needed
    ]

    # Check if the domain or any of its subdomains end with a suspicious TLD
    return any(domain_lower.endswith(tld) or domain_lower.endswith(f"{tld}.") for tld in suspicious_tlds)


# Example usage:
url_to_check = "http://www.sinduscongoias.com.br/index.php/institucional.1"
is_malicious_url(url_to_check)
