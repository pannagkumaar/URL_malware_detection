import re
from urllib.parse import urlparse
import requests
import difflib
from datetime import datetime

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import pickle
import time
import warnings
from urllib.parse import urlparse
import re
from bs4 import BeautifulSoup as bsoup
from rich.console import Console
from rich.table import Table
from fake_useragent import UserAgent
from colorama import Fore, init
import os 
from dotenv import load_dotenv

load_dotenv()
virus_total_api_key = os.getenv("VIRUS_TOTAL_API_KEY")
urlscan_api_key = os.getenv("URLSCAN_API_KEY")

init()

def urlscan_scan(url_to_scan, api_key):
    console = Console()
    def printc(message, style=None):
        if style:
            console.print(message, style=style)
        else:
            console.print(message)
    def process_urlscan_response(response_json):
        verdict_overall = response_json['verdicts']['overall']
        verdict_urlscan = response_json['verdicts']['urlscan']

        if verdict_overall['score'] > 0:
            printc(f"\n[green3][-][/green3]\nVerdict overall\n{'-'*20}")
            printc(f"[green3][-][/green3]\nTime: {response_json['task']['time']}")
            
            for verdict_overall_property, verdict_overall_value in verdict_overall.items():
                if isinstance(verdict_overall_value, list):
                    printc(f"[gold][!][/gold]{verdict_overall_property}:  {verdict_overall_value[0]}")
                else:
                    printc(f"[gold][!][/gold]{verdict_overall_property}:  {verdict_overall_value}")

            
            printc(f"\n[green3][-][/green3]\nVerdict urlscan\n{'-'*20}")
            for verdict_urlscan_property, verdict_urlscan_value in verdict_urlscan.items():
                if isinstance(verdict_urlscan_value, list):
                    if verdict_urlscan_property == 'brands':
                        for brand_key, brand_value in verdict_urlscan_value[0].items():
                            if brand_value != "":
                                print(f"[gold][!][/gold]{brand_key}:  {brand_value}")
                            else:
                                print(f"[-]{brand_key}:  N/A")
                else:
                    if verdict_urlscan_property in ['score', 'malicious']:
                        print(f"[gold][!][/gold]{verdict_urlscan_property}:  {verdict_urlscan_value}")

            printc(f"[gold][!][/gold] For more information about the report, you can check the link below ↓")
            printc(f"[green3][-][/green3]{response_json['task']['reportURL']}")
        else:
            printc(f"\n[gold][!][/gold] Verdict urlscan\n{'-'*20}")
            printc(f"[gold][!][/gold] Score: {verdict_urlscan['score']}")
            printc(f"[gold][!][/gold] Malicious: {verdict_urlscan['malicious']}")
            
            printc(f"\n[gold][!][/gold] Verdict Overall\n{'-'*20}")
            printc(f"[gold][!][/gold] Score: {verdict_overall['score']}")
            printc(f"[gold][!][/gold] Malicious: {verdict_overall['malicious']}")
            
            printc(f"[green3][-][/green3] For more information about the report, you can check the link below ↓")
            printc(f"[green3][-][/green3]{response_json['task']['reportURL']}")

    # API endpoint for scan submission
    submission_url = "https://urlscan.io/api/v1/scan/"

    # Set headers with API key
    headers = {
        "Content-Type": "application/json",
        "API-Key": api_key
    }

    # Data for scan submission
    data = {
        "url": url_to_scan,
        "visibility": "public",
        "tags": ["example_tag"]
    }

    # Submit the URL for scanning
    response = requests.post(submission_url, json=data, headers=headers)

    # Check if the submission was successful
    if response.status_code == 200:
        result_data = response.json()
        scan_uuid = result_data["uuid"]
        result_url = result_data["result"]
        
        print(f"Scan submitted successfully! Scan UUID: {scan_uuid}")
        print(f"Check results at: {result_url}")

        # Polling for scan results
        result_api_url = f"https://urlscan.io/api/v1/result/{scan_uuid}/"
        max_wait_time = 300  # Maximum wait time in seconds
        start_time = time.time()  # Initialize start_time

        while True:
            result_response = requests.get(result_api_url)
            result_status = result_response.status_code

            if result_status == 200:
                result_details = result_response.json()
                print("Scan completed successfully!")
                process_urlscan_response(result_details)
                break
            elif result_status == 404:
                time.sleep(5)
            elif result_status == 410:
                print("Scan result has been deleted.")
                break
            else:
                print(f"Unexpected error: {result_status}")
                break

            if time.time() - start_time > max_wait_time:
                print("Maximum wait time exceeded. Exiting.")
                break
    else:
        print(f"Error submitting the scan. Status Code: {response.status_code}")
        print("Error Details:", response.text)
def check_virustotal(target_url, api_key) :
    console = Console()
    
    def printc(message, style=None):
        if style:
            console.print(message, style=style)
        else:
            console.print(message)

    url = "https://www.virustotal.com/api/v3/urls"
    payload = f"url={target_url}"
    headers = {
        "accept": "application/json",
        "x-apikey": api_key,
        "content-type": "application/x-www-form-urlencoded"
    }
    max_wait_time = 60
    wait_time = 10
    elapsed_time = 0

    response = requests.post(url, data=payload, headers=headers)

    if response.status_code == 200:
        url_scan_link = response.json()['data']['links']['self']
        while elapsed_time < max_wait_time:
            url_analysis_report = requests.get(url_scan_link, headers=headers)

            if url_analysis_report.status_code == 200:
                url_analysis_report_json = url_analysis_report.json()
                url_analysis_report_id = url_analysis_report_json['meta']['url_info']['id']
                total_number_of_vendors = len(url_analysis_report_json['data']['attributes']['results'].keys())
                url_report_gui = "https://www.virustotal.com/gui/url/" + url_analysis_report_id
                url_scan_stats = url_analysis_report_json['data']['attributes']['stats']
                malicious_stats = url_scan_stats['malicious']
                results = url_analysis_report_json['data']['attributes']['results']

                if total_number_of_vendors > 0:
                    if malicious_stats > 0:
                        printc(f"[gold1][!][/gold1] [red3]{malicious_stats} security vendors flagged this URL as malicious[/red3]")
                    else:
                        printc(f"[spring_green2][+][/spring_green2] No security vendors flagged this URL as malicious")

                    printc(f"[spring_green2][+][/spring_green2] Security vendors' analysis\n{'-'*32}")

                
                    for stat, stat_value in url_scan_stats.items():
                        printc(f"[gold1][!][/gold1] {stat}: {stat_value}/{total_number_of_vendors}")

                        if malicious_stats > 0:
                            table = Table(title="𝔻 𝔼 𝕋 𝔸 𝕀 𝕃 𝕊", show_lines=True)
                            table.add_column("VENDOR", justify="center", max_width=60)
                            table.add_column("RESULT", justify="center")
                            table.add_column("METHOD", justify="center")
                            for key, value in results.items():
                                if value['category'] == "malicious":
                                    table.add_row(key, value['result'], value['method'])
                            printc(table)
                    

                    printc(f"[spring_green2][+][/spring_green2] For more information, you can check the link below ↓")
                    printc(f"[spring_green2][+][/spring_green2] {url_report_gui}")
                    break
                else:
                    printc(f"[gold1][!][/gold1] Scan still in progress. Waiting for {wait_time} seconds...")
                    time.sleep(wait_time)
                    elapsed_time += wait_time
                    wait_time = 5
            else:
                printc(f"[red3][-][/red3] {url_analysis_report.text}")
    else:
        printc(f"[red3][-][/red3] {response.text}")
        
def check_redirects(url, verbosity=True):
    class URLRedirectionChecker:
        def __init__(self, url):
            self.url = url
            self.expanded_url = None
            self.target_ip_address = None
            self.servers = []

        def get_user_agent(self):
            ua = UserAgent()
            return ua.random

        def get_domain_name(self, url: str) -> str:
            url_parts = url.split('/')
            return url_parts[2]

        def check_url_redirections(self, verbosity=True):
            headers = {
                'Accept-Encoding': 'gzip, deflate, br',
                'User-Agent': self.get_user_agent(),
                'Referer': 'https://iplogger.org/',
                'DNT': '1',
                'Upgrade-Insecure-Requests': '1',
            }

            ip_logger_url_checker = "https://iplogger.org/url-checker/"

            with requests.Session() as session:
                try:
                    response = session.get(ip_logger_url_checker, headers=headers)
                    if 'Set-Cookie' in response.headers:
                        headers['Cookie'] = response.headers['Set-Cookie']
                    if 'Cache-Control' in response.headers:
                        headers['Cache-Control'] = response.headers['Cache-Control']
                    if 'Last-Modified' in response.headers:
                        headers['If-Modified-Since'] = response.headers['Last-Modified']

                    params = {"url": self.url}
                    response = session.get(ip_logger_url_checker, headers=headers, params=params)

                    self.servers = []
                    if response.ok:
                        soup = bsoup(response.content, 'html.parser')
                        servers_info = soup.find_all("div", class_="server-info")

                        for server_info in servers_info:
                            server_items = server_info.find_all("div", class_="server-item")
                            server_antivirus = server_info.find("div", class_="server-antivirus")
                            server_next = server_info.find("div", class_="server-next")
                            server_item_info = {}

                            for server_item in server_items:
                                if server_item.contents and len(server_item.contents) == 2:
                                    key, value = server_item.contents[0].string.strip(), server_item.contents[1].string.strip()
                                    server_item_info[key] = value

                            server_item_info["Status code"] = server_next.contents[1].string.strip()
                            server_item_info["Google Safe Browsing Database"] = server_antivirus.contents[1].string.strip()
                            self.servers.append(server_item_info)

                    self.display_results(verbosity)
                except requests.RequestException as e:
                    print(f"Error checking redirects for {self.url}: {str(e)}")

        def display_results(self, verbosity):
            number_of_redirections = len(self.servers)
            console = Console()

            if verbosity and number_of_redirections > 1:
                table = Table(title="ℝ 𝔼 𝔻 𝕀 ℝ 𝔼 ℂ 𝕋 𝕀 𝕆 ℕ 𝕊", show_lines=True)
                table.add_column("ID", justify="center")
                table.add_column("URL", justify="center", max_width=60)
                table.add_column("Status Code", justify="center")
                table.add_column("IP Address", justify="center")
                table.add_column("Country by IP", justify="center")

                for server_index, server_info in enumerate(self.servers, start=1):
                    table.add_row(
                        str(server_index),
                        server_info.get('Host', ''),
                        server_info.get('Status code', ''),
                        server_info.get('IP address', ''),
                        server_info.get('Country by IP', '')
                    )

                console.print(table)
            elif number_of_redirections > 1:
                table = Table(title="ℝ 𝔼 𝔻 𝕀 ℝ 𝔼 ℂ 𝕋 𝕀 𝕆 ℕ 𝕊", show_lines=True)
                table.add_column("Source URL", justify="center", max_width=60)
                table.add_column("Source Domain", justify="center")
                table.add_column("Destination URL", justify="center", max_width=60)
                table.add_column("Destination Domain", justify="center")
                table.add_row(
                    self.url,
                    self.get_domain_name(self.url),
                    self.expanded_url if self.expanded_url else '',
                    self.get_domain_name(self.expanded_url) if self.expanded_url else ''
                )

                console.print(table)
            else:
                console.print('[green3][green3][-][/green3][/green3] No redirection found!')

    # Instantiate and use the class
    checker = URLRedirectionChecker(url)
    checker.check_url_redirections(verbosity)

def sanitization(web):
    web = web.lower()
    tokens = set()
    raw_tokens = set()

    for part in web.split('/'):
        raw_tokens.update(part.split('-'))
        raw_tokens.update(part.split('.'))

    tokens.update(raw_tokens)
    tokens.discard('com')

    return list(tokens)

def load_model(file_path):
    with open(file_path, 'rb') as file:
        model = pickle.load(file)
    return model

def load_vectorizer(file_path):
    with open(file_path, 'rb') as file:
        vectorizer = pickle.load(file)
    return vectorizer

def predict_url(model, vectorizer, url):
    x = vectorizer.transform([url])
    prediction = model.predict(x)[0]
    return prediction

def predict(url_to_check):
    warnings.filterwarnings("ignore")

    

    # Whitelist
    whitelist=open("./file.txt",'r').readlines()

    # Filtering URL
    if url_to_check in whitelist:
        prediction = 'good'
    else:
        # Loading Model and Vectorizer
        model_path = "Classifier/pickel_model_lgr.pkl"
        vectorizer_path = "Classifier/pickel_vector.pkl"
        lgr_model = load_model(model_path)
        tfidf_vectorizer = load_vectorizer(vectorizer_path)

        # Predicting
        prediction = predict_url(lgr_model, tfidf_vectorizer, url_to_check)

    warnings.resetwarnings()
    return prediction
    
def is_malicious_url(url):
    console=Console()
    console.print("[*] Analyzing the URL...\n")

    # Parse the URL
    parsed_url = urlparse(url)

    prediction = predict(url)
    prediction_label =  + '[green3]GOOD[/green3]'  if prediction == 'good' else  '[red3]MALICIOUS[/red3]' 
    console.print(f"[red3][-][/red3] The entered domain is: {prediction_label} according to our model.")
    if prediction != 'good':
        console.print("[green3][-][/green3] If you feel that this prediction is wrong or are unsure about the output, you can contact us at harshith007varma007@gmail.com or pannag2003@gmail.com. We'll check the URL and update the model accordingly. Thank you.")

    console.print(f"[gold1][!][/gold1] Starting virustotal scan...")
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")
    check_virustotal(url, virus_total_api_key)
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")
    console.print(f"[gold1][!][/gold1] Starting urlscan scan...")
    urlscan_scan(url, urlscan_api_key)
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")

    # Check for phishing keywords
    potential_typosquatting_urls = check_typosquatting("file.txt", url)

    if potential_typosquatting_urls:
        console.print("[red3][+][/red3] Potential typosquatting URLs found:")
        for entry in potential_typosquatting_urls:
            console.print(f"    - URL: {entry['url']} (Similarity: {entry['similarity']:.2f})")
    else:
        console.print("[green3][-][/green3] No potential typosquatting URLs found.")
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")    
    phishing_keywords = ['login', 'password', 'account', 'verify', 'secure']
    phishing_result = any(keyword in parsed_url.path.lower() for keyword in phishing_keywords)
    console.print(f"[green3][-][/green3] Phishing keywords check: {'Detected' if phishing_result else 'Not detected'}")
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")
    # Check for IP address in the URL
    ip_result = bool(re.match(r'\d+\.\d+\.\d+\.\d+', parsed_url.netloc))
    console.print(f"[green3][-][/green3] IP address check: {'Detected' if ip_result else 'Not detected'}")
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")
    # Check for non-standard ports
    port_result = parsed_url.port and parsed_url.port not in [80, 443]
    console.print(f"[green3][-][/green3] Non-standard ports check: {'Detected' if port_result else 'Not detected'}")
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")
    # Check for unusual URL structure
    structure_result = check_unusual_structure(parsed_url.path)
    console.print(f"[green3][-][/green3] Unusual URL structure check: {'Detected' if structure_result else 'Not detected'}")
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")
    # Check for redirect chains
    check_redirects(url)
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")
    # Check for HTTPS usage anomalies
    https_anomalies_result = check_https_anomalies(url)
    console.print(f"[green3][-][/green3] HTTPS anomalies check: {'Detected' if https_anomalies_result else 'Not detected'}")
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")
    # Check for dynamic parameters
    dynamic_params_result = check_dynamic_parameters(parsed_url.query)
    console.print(f"[green3][-][/green3] Dynamic parameters check: {'Detected' if dynamic_params_result else 'Not detected'}")
    console.print(f"[gold1]-------------------------------------------------------[/gold1]")

def check_typosquatting(file_path, input_url, threshold=0.75):
    if input_url.startswith("http://"):
        input_url = input_url[7:]
    elif input_url.startswith("https://"):
        input_url = input_url[8:]
       
    def sequence_matcher_similarity(text1, text2):
        seq_matcher = difflib.SequenceMatcher(None, text1, text2)
        return seq_matcher.ratio()

    with open(file_path, "r") as file:
        urls = [line.strip() for line in file.readlines()]
        

    input_url = input_url.lower().replace("www.", "")

    potential_typosquatting = []

    for url in urls:
        url = url.lower().replace("www.", "")
        similarity = sequence_matcher_similarity(input_url, url)
        if similarity >= threshold:
            potential_typosquatting.append({"url": url, "similarity": similarity})

    return potential_typosquatting

def has_shortening_service(url):
    pattern = re.compile(r'https?://(?:www\.)?(?:\w+\.)*(\w+)\.\w+', re.IGNORECASE)
    match = pattern.search(url)
    
    if match:
        domain = match.group(1).lower()
        common_shortening_services = {'bit', 'goo', 'tinyurl', 'ow', 't', 'is',
                                      'cli', 'yfrog', 'migre', 'ff', 'url4', 'twit',
                                      'su', 'snipurl', 'short', 'BudURL', 'ping', 
                                      'post', 'Just', 'bkite', 'snipr', 'fic', 
                                      'loopt', 'doiop', 'short', 'kl', 'wp', 
                                      'rubyurl', 'om', 'to', 'bit', 't', 'lnkd', 
                                      'db', 'qr', 'adf', 'goo', 'bitly', 'cur', 
                                      'tinyurl', 'ow', 'bit', 'ity', 'q', 'is', 
                                      'po', 'bc', 'twitthis', 'u', 'j', 'buzurl', 
                                      'cutt', 'u', 'yourls', 'x', 'prettylinkpro', 
                                      'scrnch', 'filoops', 'vzturl', 'qr', '1url', 
                                      'tweez', 'v', 'tr', 'link', 'zip'}
        
        if domain in common_shortening_services:
            return 1
    return 0




def check_unusual_structure(path):
    # Check for unusual URL structure
    # Implement your own logic based on path analysis

    # 1. Long Sequences of Repeating Characters
    consecutive_repeating_chars = re.search(r'(.)\1{5,}', path)
    if consecutive_repeating_chars:
        return True

    # 2. Uncommon Symbols
    uncommon_symbols = set(path) - set('abcdefghijklmnopqrstuvwxyz0123456789-._/')
    if uncommon_symbols:
        return True

    # 3. Excessive Use of Hyphens or Underscores
    excessive_hyphens_underscores = re.search(r'[-_]{4,}', path)
    if excessive_hyphens_underscores:
        return True

    # 4. Unusual Path Length
    if len(path) > 50:
        return True

    # 5. Presence of Hexadecimal Encoding
    if re.search(r'%[0-9A-Fa-f]{2}', path):
        return True

    # 6. Uncommon File Extensions
    uncommon_extensions = re.search(r'\.\w{4,}', path)
    if uncommon_extensions:
        return True

    # If none of the checks found anything unusual
    return False





def check_https_anomalies(url):
    console=Console()
    try:
        # Ensure the URL starts with 'https://'
        if not url.startswith('https://'):
            console.print("[gold][!][/gold] URL does not use HTTPS.")
            return True

        # Retrieve SSL certificate
        response = requests.head(url, allow_redirects=True, timeout=5)
        if response.status_code == 200 and response.url.startswith('https://'):
            parsed_url = urlparse(url)
            domain = parsed_url.netloc

            # Check if the SSL certificate is valid
            cert = response.connection.sock.getpeercert()
            not_after = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
            if not_after < datetime.now():
                print("SSL certificate has expired.")
                return True

            # Check if the common name matches the domain
            common_name = cert['subject'][0][0][1]
            if not common_name.lower() == domain.lower():
                print(f"Common name in SSL certificate does not match the domain: {common_name}")
                return True

            # Check if the certificate is issued by a trusted authority
            issuer = cert['issuer'][0][0][1]
            trusted_authorities = ['Let\'s Encrypt', 'DigiCert', 'Comodo', 'Symantec', 'GeoTrust']
            if not any(authority.lower() in issuer.lower() for authority in trusted_authorities):
                print(f"Certificate issuer is not from a trusted authority: {issuer}")
                return True

            # Add more checks as needed based on your specific requirements

    except requests.exceptions.RequestException as e:
        print(f"Error checking HTTPS anomalies: {e}")

    return False

def check_dynamic_parameters(query, threshold=3):
    # Check for dynamic parameters
    # Implement more sophisticated logic based on query analysis

    # Split the query string into individual parameters
    parameters = [param.split('=') if '=' in param else (param, '') for param in query.split('&')]

    # Check the total number of parameters
    if len(parameters) > threshold:
        return True

    # Check for specific patterns in parameter names or values
    for param in parameters:
        # Ensure that the parameter has both a name and a value
        if len(param) == 2:
            # Check for numeric values (considering decimals)
            if re.match(r'^[+-]?\d*\.?\d+$', param[1]):
                return True

            # Check for suspicious characters in parameter names or values
            if re.search(r'[^\w.-]', param[0]) or re.search(r'[^\w.-]', param[1]):
                return True

            # Improve SQL injection detection
            if re.search(r'\b(union|select|insert|update|delete|alter|drop|truncate|exec)\b', param[1], re.IGNORECASE):
                return True

            # Enhance XSS detection
            if re.search(r'<\s*/?\s*script\s*>', param[1], re.IGNORECASE):
                return True

            # Check for parameter names commonly used in attacks
            if param[0].lower() in ['script', 'exec', 'cmd', 'system', 'eval']:
                return True

            # Check for excessively long parameter values
            if len(param[1]) > 50:
                return True

        # Add more checks based on your specific requirements

    # If none of the checks found anything suspicious
    return False

def is_suspicious_tld(domain):
    # Make the comparison case-insensitive
    domain_lower = domain.lower()

    # List of suspicious TLDs
    suspicious_tlds = [
        '.tk', '.ml', '.ga', '.cf', '.gq',
        '.pw', '.to', '.cc', '.ws', '.su',
        '.buzz', '.download', '.guru', '.loan', '.racing',
        '.review', '.top', '.vip', '.win', '.work',
        '.country', '.stream', '.download', '.xin', '.gdn',
        '.jetxt', '.bid', '.vip', '.ren', '.kim',
        '.loan', '.mom', '.party', '.review', '.trade',
        '.date', '.wang', '.accountants'
        # Add more TLDs as needed
    ]

    # Check if the domain or any of its subdomains end with a suspicious TLD
    return any(domain_lower.endswith(tld) or domain_lower.endswith(f"{tld}.") for tld in suspicious_tlds)


# Example usage:
url_to_check = "http://www.sinduscongoias.com.br/index.php/institucional.1"
is_malicious_url(url_to_check)
