import re
from urllib.parse import urlparse
import requests
import difflib
from datetime import datetime

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import pickle
import warnings
from urllib.parse import urlparse
import re
from colorama import Fore, init
init()
def sanitization(web):
    web = web.lower()
    tokens = set()
    raw_tokens = set()

    for part in web.split('/'):
        raw_tokens.update(part.split('-'))
        raw_tokens.update(part.split('.'))

    tokens.update(raw_tokens)
    tokens.discard('com')

    return list(tokens)

def load_model(file_path):
    with open(file_path, 'rb') as file:
        model = pickle.load(file)
    return model

def load_vectorizer(file_path):
    with open(file_path, 'rb') as file:
        vectorizer = pickle.load(file)
    return vectorizer

def predict_url(model, vectorizer, url):
    x = vectorizer.transform([url])
    prediction = model.predict(x)[0]
    return prediction

def predict(url_to_check):
    warnings.filterwarnings("ignore")

    

    # Whitelist
    whitelist=open("./file.txt",'r').readlines()

    # Filtering URL
    if url_to_check in whitelist:
        prediction = 'good'
    else:
        # Loading Model and Vectorizer
        model_path = "Classifier/pickel_model_lgr.pkl"
        vectorizer_path = "Classifier/pickel_vector.pkl"
        lgr_model = load_model(model_path)
        tfidf_vectorizer = load_vectorizer(vectorizer_path)

        # Predicting
        prediction = predict_url(lgr_model, tfidf_vectorizer, url_to_check)

    warnings.resetwarnings()
    return prediction
    

def is_malicious_url(url):
    print("[*] Analyzing the URL...\n")

    # Parse the URL
    parsed_url = urlparse(url)

    prediction = predict(url)
    print(f"[+] The entered domain is: {Fore.GREEN + 'GOOD' + Fore.RESET if prediction == 'good' else Fore.RED + 'MALICIOUS' + Fore.RESET}")

    if prediction != 'good':
        print("[-] If you feel that this prediction is wrong or are unsure about the output, you can contact us at harshith007varma007@gmail.com or pannag2003@gmail.com. We'll check the URL and update the model accordingly. Thank you.")

    # Check for phishing keywords
    potential_typosquatting_urls = check_typosquatting("file.txt", url)

    if potential_typosquatting_urls:
        print("[-] Potential typosquatting URLs found:\n")
        for entry in potential_typosquatting_urls:
            print(f"    - URL: {entry['url']} (Similarity: {entry['similarity']:.2f})")
    else:
        print("[+] No potential typosquatting URLs found.\n")
        
    phishing_keywords = ['login', 'password', 'account', 'verify', 'secure']
    phishing_result = any(keyword in parsed_url.path.lower() for keyword in phishing_keywords)
    print(f"[-] Phishing keywords check: {'Detected' if phishing_result else 'Not detected'}\n")

    # Check for IP address in the URL
    ip_result = bool(re.match(r'\d+\.\d+\.\d+\.\d+', parsed_url.netloc))
    print(f"[-] IP address check: {'Detected' if ip_result else 'Not detected'}\n")

    # Check for non-standard ports
    port_result = parsed_url.port and parsed_url.port not in [80, 443]
    print(f"[-] Non-standard ports check: {'Detected' if port_result else 'Not detected'}\n")

    # Check for unusual URL structure
    structure_result = check_unusual_structure(parsed_url.path)
    print(f"[-] Unusual URL structure check: {'Detected' if structure_result else 'Not detected'}\n")

    # Check for redirect chains
    redirects_result = has_redirects(url)
    print(f"[-] Redirect chains check: {'Detected' if redirects_result else 'Not detected'}\n")

    # Check for HTTPS usage anomalies
    https_anomalies_result = check_https_anomalies(url)
    print(f"[-] HTTPS anomalies check: {'Detected' if https_anomalies_result else 'Not detected'}\n")

    # Check for dynamic parameters
    dynamic_params_result = check_dynamic_parameters(parsed_url.query)
    print(f"[-] Dynamic parameters check: {'Detected' if dynamic_params_result else 'Not detected'}\n")

def check_typosquatting(file_path, input_url, threshold=0.75):
    if input_url.startswith("http://"):
        input_url = input_url[7:]
    elif input_url.startswith("https://"):
        input_url = input_url[8:]
       
    def sequence_matcher_similarity(text1, text2):
        seq_matcher = difflib.SequenceMatcher(None, text1, text2)
        return seq_matcher.ratio()

    with open(file_path, "r") as file:
        urls = [line.strip() for line in file.readlines()]
        

    input_url = input_url.lower().replace("www.", "")

    potential_typosquatting = []

    for url in urls:
        url = url.lower().replace("www.", "")
        similarity = sequence_matcher_similarity(input_url, url)
        if similarity >= threshold:
            potential_typosquatting.append({"url": url, "similarity": similarity})

    return potential_typosquatting

def has_shortening_service(url):
    pattern = re.compile(r'https?://(?:www\.)?(?:\w+\.)*(\w+)\.\w+', re.IGNORECASE)
    match = pattern.search(url)
    
    if match:
        domain = match.group(1).lower()
        common_shortening_services = {'bit', 'goo', 'tinyurl', 'ow', 't', 'is',
                                      'cli', 'yfrog', 'migre', 'ff', 'url4', 'twit',
                                      'su', 'snipurl', 'short', 'BudURL', 'ping', 
                                      'post', 'Just', 'bkite', 'snipr', 'fic', 
                                      'loopt', 'doiop', 'short', 'kl', 'wp', 
                                      'rubyurl', 'om', 'to', 'bit', 't', 'lnkd', 
                                      'db', 'qr', 'adf', 'goo', 'bitly', 'cur', 
                                      'tinyurl', 'ow', 'bit', 'ity', 'q', 'is', 
                                      'po', 'bc', 'twitthis', 'u', 'j', 'buzurl', 
                                      'cutt', 'u', 'yourls', 'x', 'prettylinkpro', 
                                      'scrnch', 'filoops', 'vzturl', 'qr', '1url', 
                                      'tweez', 'v', 'tr', 'link', 'zip'}
        
        if domain in common_shortening_services:
            return 1
    return 0




def check_unusual_structure(path):
    # Check for unusual URL structure
    # Implement your own logic based on path analysis

    # 1. Long Sequences of Repeating Characters
    consecutive_repeating_chars = re.search(r'(.)\1{5,}', path)
    if consecutive_repeating_chars:
        return True

    # 2. Uncommon Symbols
    uncommon_symbols = set(path) - set('abcdefghijklmnopqrstuvwxyz0123456789-._/')
    if uncommon_symbols:
        return True

    # 3. Excessive Use of Hyphens or Underscores
    excessive_hyphens_underscores = re.search(r'[-_]{4,}', path)
    if excessive_hyphens_underscores:
        return True

    # 4. Unusual Path Length
    if len(path) > 50:
        return True

    # 5. Presence of Hexadecimal Encoding
    if re.search(r'%[0-9A-Fa-f]{2}', path):
        return True

    # 6. Uncommon File Extensions
    uncommon_extensions = re.search(r'\.\w{4,}', path)
    if uncommon_extensions:
        return True

    # If none of the checks found anything unusual
    return False



def has_redirects(url, max_redirects=5, timeout=5, user_agents=None, whitelist=None):
    if user_agents is None:
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
            # Add more user agents as needed
        ]

    headers = {'User-Agent': user_agents[0]}

    try:
        response = requests.head(url, allow_redirects=True, timeout=timeout, headers=headers)

        # Check if there are multiple redirects
        if len(response.history) > max_redirects:
            return True

        # Check the final HTTP status code
        if not (200 <= response.status_code < 300):
            return False

        # Follow all redirects and check the final destination
        final_url = response.url
        if whitelist and any(domain in final_url for domain in whitelist):
            return False

        # Implement additional checks, such as pattern matching or threat intelligence integration

    except requests.RequestException as e:
        # Handle request exceptions (e.g., connection timeout, invalid URL)
        print(f"Error checking redirects for {url}: {str(e)}")

    return False

def check_https_anomalies(url):
    try:
        # Ensure the URL starts with 'https://'
        if not url.startswith('https://'):
            print("URL does not use HTTPS.")
            return True

        # Retrieve SSL certificate
        response = requests.head(url, allow_redirects=True, timeout=5)
        if response.status_code == 200 and response.url.startswith('https://'):
            parsed_url = urlparse(url)
            domain = parsed_url.netloc

            # Check if the SSL certificate is valid
            cert = response.connection.sock.getpeercert()
            not_after = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
            if not_after < datetime.now():
                print("SSL certificate has expired.")
                return True

            # Check if the common name matches the domain
            common_name = cert['subject'][0][0][1]
            if not common_name.lower() == domain.lower():
                print(f"Common name in SSL certificate does not match the domain: {common_name}")
                return True

            # Check if the certificate is issued by a trusted authority
            issuer = cert['issuer'][0][0][1]
            trusted_authorities = ['Let\'s Encrypt', 'DigiCert', 'Comodo', 'Symantec', 'GeoTrust']
            if not any(authority.lower() in issuer.lower() for authority in trusted_authorities):
                print(f"Certificate issuer is not from a trusted authority: {issuer}")
                return True

            # Add more checks as needed based on your specific requirements

    except requests.exceptions.RequestException as e:
        print(f"Error checking HTTPS anomalies: {e}")

    return False

def check_dynamic_parameters(query, threshold=3):
    # Check for dynamic parameters
    # Implement more sophisticated logic based on query analysis

    # Split the query string into individual parameters
    parameters = [param.split('=') if '=' in param else (param, '') for param in query.split('&')]

    # Check the total number of parameters
    if len(parameters) > threshold:
        return True

    # Check for specific patterns in parameter names or values
    for param in parameters:
        # Ensure that the parameter has both a name and a value
        if len(param) == 2:
            # Check for numeric values (considering decimals)
            if re.match(r'^[+-]?\d*\.?\d+$', param[1]):
                return True

            # Check for suspicious characters in parameter names or values
            if re.search(r'[^\w.-]', param[0]) or re.search(r'[^\w.-]', param[1]):
                return True

            # Improve SQL injection detection
            if re.search(r'\b(union|select|insert|update|delete|alter|drop|truncate|exec)\b', param[1], re.IGNORECASE):
                return True

            # Enhance XSS detection
            if re.search(r'<\s*/?\s*script\s*>', param[1], re.IGNORECASE):
                return True

            # Check for parameter names commonly used in attacks
            if param[0].lower() in ['script', 'exec', 'cmd', 'system', 'eval']:
                return True

            # Check for excessively long parameter values
            if len(param[1]) > 50:
                return True

        # Add more checks based on your specific requirements

    # If none of the checks found anything suspicious
    return False

def is_suspicious_tld(domain):
    # Make the comparison case-insensitive
    domain_lower = domain.lower()

    # List of suspicious TLDs
    suspicious_tlds = [
        '.tk', '.ml', '.ga', '.cf', '.gq',
        '.pw', '.to', '.cc', '.ws', '.su',
        '.buzz', '.download', '.guru', '.loan', '.racing',
        '.review', '.top', '.vip', '.win', '.work',
        '.country', '.stream', '.download', '.xin', '.gdn',
        '.jetxt', '.bid', '.vip', '.ren', '.kim',
        '.loan', '.mom', '.party', '.review', '.trade',
        '.date', '.wang', '.accountants'
        # Add more TLDs as needed
    ]

    # Check if the domain or any of its subdomains end with a suspicious TLD
    return any(domain_lower.endswith(tld) or domain_lower.endswith(f"{tld}.") for tld in suspicious_tlds)


# Example usage:
url_to_check = "http://www.sinduscongoias.com.br/index.php/institucional.1"
is_malicious_url(url_to_check)
